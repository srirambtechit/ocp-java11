AI 1
Technical Project Proposal
Transforming Live cricket coverage into highlights

Problem Description 
AI is widely accepted in the field of sports. Cricket is one of the most popular sports in the world after soccer. But compared to the other sports cricket is played for a longer duration of time. Though the shortest version of the game is played for four hours, nobody has the time and patience to watch the entire match.  Ever since the introduction of AI in cricket, AI has made the sport even more interesting with a diverse highlights packages that removes the boring part of the game and entertains the audience with only the moments that were interesting.  This project is intended to build a system that converts the live coverage of a match into short highlights.

Input and Output

In order to train the model, several hours of video montage will be fed into the system as input. Object in the video like ball, stump, bails will be detected and the metadata of these objects will be considered as input. The video will be in .mp4 format which contains information on the playing X1, fours, sixes, wickets and important milestones achieved as a part of the game.  All important elements or events of the match are tagged exhaustively and stored. The model can identify and cut these tagged events and are clustered together in the form of short highlights based on the predefined rules. For this project video footage of Cricket World Cup 2019 would be used as the input.

Evaluation Metric
The final output is evaluated based on whether the model has captured all wickets where the batsman being bowled with the help of the predefined rules. Any wickets that involves batsman getting dismissed by any other means will deem the model to be less accurate.

             No. of bowled predicted
Accuracy = ----------------------------
            Total no. of batsman bowled
						
Related works
OTT platform and sports broadcasters are finding out ways of effective utilization of Artificial Intelligence in the field of sports. Though the format and the ground rules of the game differs from one game to another, streaming platforms are looking out to provide customized summary of the match which in turn augments the viewership and revenue opportunities. Some of the big giants like esports are involved in the creation of highlights and summary generation of a live video. (Ref 1)

Methodology 
Computer vision techniques will be used to differentiate the objects and the graphical representation on the screen. Though there are number of algorithms available like YOLO, Single shot detector, Faster R-CNN, Region based convolution Neural Networks etc. each of these has its own advantages and disadvantages. Hence an algorithm that suits our model best should be taken for the identification and classification of objects. Classification would be the approach used on the data but the algorithm might change over the course of time post the deep dive analysis on the data.

Data plays a very important role in order to generate the highlights of the cricket match and the system needs at least two overs of the video footage, so this can be processed as batch rather than real time. The model should read from the metadata as inputs and detect objects from the video. For our model faster R-CNN algorithm will be used. Though YOLO has a good performance rate, it lacks accuracy whereas faster R-CNN has more accuracy which would be ideal for generating summary of the match.

Cricket as a game has many rules and important events like fours, six, wickets etc. are being captured as highlights. There are multiple ways of getting a wicket like being stumped, getting bowled, caught behind and so on. For our use case we will focus on the wickets where batsmen getting bowled and train the model with a lot of sample videos where the batsmen are dismissed in that fashion.

Description of the challenges
Some of the challenges that the model might experience are the ability to detect a ball when it is bowled at a speed of 150kmph. Unlike football, the size of the ball in cricket is very small. Losing sight of the ball when obstructed by a batsman would be highly challenging.


\begin{thebibliography}{9}
\bibitem{knuth} D. E. Knuth. {\em The \TeX~book.}\/ Addison-Wesley,
Reading, Massachusetts, 1984.
\bibitem{lamport} L. Lamport. {\em \LaTeX~: A Document Preparation
System}.\/ Addison-Wesley, Reading, Massachusetts, 1986.
\bibitem{ken} Ken Wessen, Preparing a thesis using \LaTeX~, private
communication, 1994.
\bibitem{lamport2} L. Lamport. Document Production: Visual
or Logical, {\em Notices of the Amer. Maths. Soc.},\/ Vol. 34,
1987, pp. 621-624.
\bibitem{esport} Seok-Kyu Kang, Jee-Hyong Lee {\em \LaTeX~: An E-sports Video 
Highlight Generator Using Win-Loss Probability Model}.\/ March, 2020.
\end{thebibliography}

\[Accuracy = \frac{No. of bowled  predicted}{Total no. of  batsman  bowled}\]

AI Class notes:
Tree Search  = recursion
Graph Search = recursion + memoization

Input/Output:
Full live cricket video footage
Scope, detect wicket happend to be bold

Research:
https://www.primefocustechnologies.com/blog/artificial-intelligence-transforming-the-live-sports-landscape/

Implementation approach:
try out: 1.https://www.analyticsvidhya.com/blog/2019/09/guide-automatic-highlight-generation-python-without-machine-learning/
2.https://www.analyticsvidhya.com/blog/2020/03/ball-tracking-cricket-computer-vision/
3.https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/
4.https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/
5.https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/
6.https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/
7.https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/
8.https://towardsdatascience.com/object-detection-simplified-e07aa3830954

reference:
https://machinelearningmastery.com/object-recognition-with-deep-learning/
image classification = assigning class label to an image
object localization = drawing a bounding box around one or more objects in image
object detection = object localization + image classification
object recognition = combined pbm of all 3 of the above

R-CNN = Region-Based Convolutional Neural Network for perfomance
YOLO = You Only Look Once for speed and real-time use

VOC-2012 dataset and the 200-class ILSVRC-2013 object detection dataset

techniques R-CNN, Fast R-CNN, and Faster-RCNN designed

Rich feature hierarchies for accurate object detection and semantic segmentation
https://arxiv.org/abs/1311.2524

Fast R-CNN https://arxiv.org/abs/1504.08083
uses Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition https://arxiv.org/abs/1406.4729
https://github.com/rbgirshick/fast-rcnn

Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks 
https://arxiv.org/abs/1506.01497
https://github.com/rbgirshick/py-faster-rcnn

The RPN works by taking the output of a pre-trained deep CNN, such as VGG-16

Mask R-CNN https://arxiv.org/abs/1703.06870

YOLO:
You Only Look Once; Unified, Real-Time Object Detection https://arxiv.org/abs/1506.02640

YOLO9000: Better, Faster, Stronger https://arxiv.org/abs/1612.08242

YOLOv3: An Incremental Improvement https://arxiv.org/abs/1804.02767

In order to generate highlights of cricket match, system needs at least 2 overs of the video footage so, this can processed as batch rather in real time. YOLO

Comparision
Accuracy = Fast R-CNN > YOLO
Speed    = Fast R-CNN < YOLO (in real-time)

https://blog.roboflow.com/breaking-down-efficientdet/
https://blog.roboflow.com/yolov5-improvements-and-evaluation/
https://ai.facebook.com

CV Words:
object tracking

https://www.analyticsvidhya.com/blog/2020/02/sports-analytics-generating-actionable-insights-using-cricket-commentary/

https://courses.analyticsvidhya.com/pages/all-free-courses

Advanced AI-led solutions are capable of 
	identifying and extracting metadata for specific 
		game objects
		constructs
		players 
		events 
		actions

AI mimics human recognition to discern the game
	Scene Identification
	Reading On Screen Score graphics
	Analyzing sound from the ground
	Listening to commentary

Neural networks
	CNN
	R-CNN
	LSTM
	Computer Vision techniques

Aspects of NN:	
	Object detection: Different engines focused on specific groups of game objects and formations
	Optical flow-based Computer Vision to identify movement of a ball/moving object
	Face detection to recognize players
	Scale Invariant Feature Transformation (SIFT) to detect complex objects and actions
	Audio Spectrogram to find key contact points like a ball hitting a bat/racquet
	Audio classification and excitement level classification
	Image classification
	
To train these engines, we need to sift through hundreds of hours of match footage and annotate different frames, objects, actions etc. and generate training data for ML. These cognition engines are then stitched together using game logic and understanding of sports production in order to catalogue the game, segment by segment. E.g. for cricket, we deploy more than 11 such engines to extract 25+ attributes per ball including batsman, non-striker, bowler, runs scored, type of shot, fours, sixes, replays, crowd excitement levels, celebrations, wickets, bowling type, ball synopsis etc.

state-of-the-art models on 
			ImageNet (image classification), 
			Kinetics (video classification), 
			MS COCO (object detection and instance segmentation), and 
			Cityscapes (semantic segmentation).


Image Classfication + Localization = Object Detection

Object Detection is the task of identifying an object and its location in an image. Object detection is similar to an image classification problem but with one additional task – identifying the location of an object as well – a concept known as Localization.

Object Tracking is a special case of Object Detection. It applies to only video data. In object tracking, the object and its location are identified from every frame of a video.

Video (30 fps) = 30 images / second
   F1, F2, F3, F4,...F30

A scene in video may consist 30 seconds of the length
so, processing 30 seconds leads to 30 * 30 = 900 sequential images

                  (Object Detection on Every Frames of the Video)
Object Tracking =            F1, F2, F3, F4,...F30


segmentation by color

Saving model state and resume during training phase?
How to use googlecolab to train model more than 12 hours?
any tips & trics?




https://en.wikipedia.org/wiki/Hawk-Eye

https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37109.pdf
At the feature extraction stage, frame-based low-level visual features such as the color histogram (CH) or his- togram of oriented gradients (HOG) are extracted from the video clips using relevant image processing and computer vision algorithms.

object segmentation 
instance segmentation
diarization = segmentation + clustering
Hidden Markov Models

unigram and bigram statistics of the detected events.


https://docs.opencv.org/master/d9/df8/tutorial_root.html

part-1
https://medium.com/@arifromadhan19/the-basics-of-decision-trees-e5837cc2aba7

part-2
https://medium.com/analytics-vidhya/classification-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-8e5f5228b11e
 - scikit-learn uses optimized decision tree 
   ref: https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart

part-3
https://medium.com/@arifromadhan19/regrssion-in-decision-tree-a-step-by-step-cart-classification-and-regression-tree-196c6ac9711e

https://dl.acm.org/doi/epdf/10.1145/973264.973296
generating highlights is 
	by optimizing the use of visual features
	by analyzing the temporal structure of audio-visual features.

https://dl.acm.org/doi/epdf/10.1145/3265845.3265852
Fast and Accurate Object Detection Using ImageCropping/Resizing in Multi-View 4K Sports Videos

https://dl.acm.org/doi/epdf/10.1145/3341105.3373894
An E-sports Video Highlight Generator Using Win-LossProbability Model

https://dl.acm.org/doi/10.1145/1374296.1374324
https://dl.acm.org/doi/epdf/10.1145/1374296.1374324
2006 - Semantic concept extraction from sports video for highlight generation

https://dl.acm.org/doi/10.1145/2911996.2912011
https://dl.acm.org/doi/epdf/10.1145/2911996.2912011
Using Viewer's Facial Expression and Heart Rate for Sports Video Highlights Detection

https://dl.acm.org/doi/epdf/10.1145/2647868.2654973
Event Detection based on Twitter Enthusiasm Degree forGenerating a Sports Highlight Video



https://scholar.google.ca/scholar?oi=bibs&cluster=13326821131815512459&btnI=1&hl=en
http://www.sloansportsconference.com/wp-content/uploads/2020/02/SSAC2020-Paper-1548748-Cricket-Shot-Prediction.pdf
You Cannot Do That Ben Stokes: Dynamically Predicting Shot Type in Cricket Using a Personalized Deep Neural Network


https://ieeexplore.ieee.org/document/8575397
Automatic Cricket Highlight Generation Using Event-Driven and Excitement-Based Features


https://ieeexplore.ieee.org/document/5319046
Unified Sports Video Highlight Detection Based on Multi-feature Fusion
- slow-motion replay
- detect audience cheers based on pitch frequency and short-time energy

https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8393107
Highlight Extraction in Soccer Videos by Using Multimodal Analysis

https://docs.opencv.org/master/d9/dab/tutorial_homography.html

Ref: http://ai.berkeley.edu/home.html

Journal paper
ICLR
https://scihub.wikicn.top

plagiarism upto 20% excluding reference

//////////////////////////// End of the Section ////////////////////////////////

Github repos
------------
 1.deoldify 
   https://github.com/jantic/DeOldify
 2.face-recognition
   https://github.com/ageitgey/face_recognition
 3.srez (Image super-resolution through deep learning)
   https://github.com/david-gpu/srez
 4.leon (open-source personal assistant)
   https://github.com/leon-ai/leon	 
 5.deepjazz - DL driven jazz generation using Keras & Theano! 
   https://github.com/jisungk/deepjazz
 	 https://deepjazz.io
 6.awesome-machine-learning
   https://github.com/josephmisiti/awesome-machine-learning
 7.video feature extraction (tried, it works)
   https://github.com/nicholaskajoh/ivy 
 8.Video analysis
 	 https://github.com/topics/video-analysis
 9.Non Local Neural Net for Video Classification (Facebook research)
   https://github.com/facebookresearch/video-nonlocal-net
10.Faster R-CNN and Mask R-CNN in PyTorch 1.0
   https://github.com/facebookresearch/maskrcnn-benchmark/
11.Detectron2
   https://github.com/facebookresearch/detectron2
	 https://github.com/roytseng-tw/Detectron.pytorch
12.CocoAPI
	 https://github.com/cocodataset/cocoapi
	 https://cocodataset.org/#detection-2020
//////////////////////////// End of the Section ////////////////////////////////

Reference material
------------------
1. video Deoldify new york
2. Leon https://www.youtube.com/watch?v=p7GRGiicO1c
3. Pretrained Neural net for image processing (tried, it works)
   Yolo - https://pjreddie.com/darknet/yolo/
4. Ivy is an open-source video-based object counting software for tallying
   pretty much anything (vehicles, people, animals — you name it).
   ref: https://github.com/nicholaskajoh/ivy
5. This is collection of projects and links about algorithm visualization.
   ref: https://github.com/enjalot/algovis


https://towardsdatascience.com/generate-any-sport-highlights-using-python-3695c98baead

https://www.kdnuggets.com/2020/01/7-ai-use-cases-transforming-live-sports-production-distribution.html

https://github.com/PsychoinformaticsLab/pliers

https://github.com/ricoms/video_image_features

https://github.com/snrao310/Video-Feature-Extraction

https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_table_of_contents_video/py_table_of_contents_video.html

vim editor - handy notes:
https://medium.com/free-code-camp/how-not-to-be-afraid-of-vim-anymore-ec0b7264b0ae

https://pjreddie.com/courses/computer-vision/

//////////////////////////// End of the Section ////////////////////////////////

Infrastructure
--------------
1. Managing multiple version of pythons
ref: https://medium.com/faun/pyenv-multi-version-python-development-on-mac-578736fb91aa
pyenv install --list
pyenv install 3.7.6
pyenv versions

$ pyenv local 2.7.17 3.5.9
$ python --version
Python 2.7.17
$ python3.5 -- version
Python 3.5.9

3 levels:
Shell     ->     Local     -->      Global
     (overrides)        (overrides)


2. Manageing environment with python3+
https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/
# 1.1 create virtual environment for ivy project
python3 -m venv env/ivy

# 1.2 activating environment
source env/bin/activate

# 1.3 checking python pointing to our environment
which python or python3

# 1.4 leave virtual environment
deactivate

# 1.5 install package using pip
pip install pkg
(or)
pip install -r requirements.txt
(or)
pip install package.tar.gz

# 1.6 list out all package
pip freeze > requirements.txt


3. [Un]Linking virtual environment with jupyter notebook
# 1.1 activate virtual environment first
source myenv/bin/activate

# 1.2 install ipykernal
pip install ipykernel
(or if global)
pip install --user ipykernel

# 1.3.1 add your virtual environment to Jupyter
python -m ipykernel install --user --name=myenv
# This should print the following
# Installed kernelspec myenv in /home/user/.local/share/jupyter/kernels/myenv

# 1.3.2 if any error, reinstall jupyterlab
pip install jupyterlab # this ensures jupyter installed on env again

# 1.4.1 if you want to unlink, lets list those
jupyter kernelspec list
# Available kernels:
#   myenv      /home/user/.local/share/jupyter/kernels/myenv
#   python3    /usr/local/share/jupyter/kernels/python3

# 1.4.2 now uninstall
jupyter kernelspec uninstall myenv

# Misc
# Trusting notebooks created by others https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#trusting-notebooks
jupyter trust mynotebook.ipynb

# Managing with Miniconda environment
https://towardsdatascience.com/creating-a-solid-data-science-development-environment-60df14ce3a34?gi=ec4324ac19be

Tools:
Git
DVC
Conda
JupyterLab

# 1) Install Git
sudo apt-get install git
# 2) Configure your Git account
git config --global user.name "Your Name" 
git config --global user.email "yourmail@mail.com"

# 3) Download Miniconda latest release for Linux
wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
# 4) Change the permission to run the Miniconda bash file
chmod +x Miniconda3-latest-Linux-x86_64.sh
# 5) Run Miniconda installation file
./Miniconda3-latest-Linux-x86_64.sh
# 6) Export the path to Miniconda installation folder
export PATH=/home/YOURNAME/miniconda3/bin:$PATH

# 8) Define the your GitHub information as variables
GitHubName=<YourGitHubName>
GitHubPassword=<YourGitHubPassword>
# 9) Create a new git repository on GitHub 
#    named "DataScience_DevEnv"
curl -u $GitHubName:$GitHubPassword https://api.github.com/user/repos -d '{"name":"DataScience_DevEnv"}'
# 10) Check if your new repository is available on GitHub
curl "https://api.github.com/users/$GitHubName/repos?per_page=100" | grep -w clone_url | grep -o '[^"]\+://.\+.git'
# 11) Create a folder with the name of your repository
mkdir DataScience_DevEnv
cd DataScience_DevEnv
# 12) Create a README file for your repository
echo "# Data Science development environment repository" >> README.md
# 13) Initiate our local Git repository
git init
# 14) Add, commit and push README.md to GitHub
git add README.md
git commit -m "first commit with README file"
git remote add origin https://github.com/GabrielSGoncalves/DataScience_DevEnv.git
git push -u origin master

# 15) Create o Conda environment
conda create -n datascience_devenv python=3.7 pandas scikit-learn
# 16) Activate your environment
conda activate datascience_devenv

# 16.1) rename conda environment
conda create --name new_name --clone old_name
conda remove --name old_name --all # or its alias: `conda env remove --name old_name`

# 17) Install JupyterLab with
# conda
conda install -c conda-forge jupyterlab
# or pip
pip install jupyterlab
# 18) Install DVC with
# conda
conda install -c conda-forge dvc
# or pip
pip install dvc

# 19) List your packages installed
# with conda
conda list
# with pip
pip list
# 20) Create requirements file
# with conda
conda list --export > requirements.txt
# with pip
pip freeze > requirements.txt

# 21) Install DVC and its dependecies for connection with S3
pip install dvc[s3]
# 22) Initialize DVC repository
dvc init
# 23) Create folder on repository to store data files
mkdir data
# 24) Create S3 bucket
aws s3 mb s3://dvc-datascience-devenv
# 25) Define the new bucket as remote storage for DVC
dvc remote add -d myremote s3://dvc-datascience-devenv
# 26) List your DVC remote folder
dvc remote list 
# 27) Download data file
wget -P data/ https://dvc-repos-gsg.s3.amazonaws.com/models_pytorch_n_params.csv
# 28) Add data file to DVC
dvc add data/models_pytorch_n_params.csv

# 29) Start tracking DVC file and .gitignore with Git
git add data/.gitignore data/models_pytorch_n_params.csv.dvc
git commit -m "Start versioning csv file stored with DVC on S3 bucket"
git push
# 30) Push data file to DVC remote storage on S3 bucket
dvc push

# 31) Install ipython using conda
conda install ipykernel
# 32) Install your kernel based on your working environment
ipython kernel install --user --name=datascience_devenv
# 33) List the kernels you have available
jupyter kernelspec list

# 34) To export your current conda environment to YAML
conda env export > datascience_devenv.yaml
# 35) Add the yaml file to our GitHub repository
git add datascience_devenv.yaml
git commit -m 'add environment yaml to repo'
git push

# 36) Project repository structure
tree
.
├── data
│   ├── models_pytorch_n_params.csv
│   └── models_pytorch_n_params.csv.dvc
├── datascience_devenv.yaml
├── README.md
└── requirements.txt

# 37) Detailed repository structure
tree -a
.
├── data
│   ├── .gitignore
│   ├── models_pytorch_n_params.csv
│   └── models_pytorch_n_params.csv.dvc
├── datascience_devenv.yaml
├── .dvc
│   ├── cache
│   │   └── 6f
│   │       └── 387350081297a29ecde86ebfdf632c
│   ├── config
│   ├── .gitignore
│   .
│   .
├── .git
│   ├── branches
│   .
│   .
│   .
├── README.md
└── requirements.txt

# 38) Add .gitignore for script files on our repository
echo "*.pyc" >> .gitignore
git add .gitignore
git commit -m 'Add .gitignore for regular files'
git push

5. Replicating our development environment
# 39) On a new machine, clone the repository
git clone https://github.com/$GitHubName/DataScience_DevEnv.git
# 40) Create conda environment
conda env create --file=datascience_devenv.yaml
# 41) Activate environment
conda activate datascience_devenv
# 42) Install the JupyterLab kernel
ipython kernel install --user --name=datascience_devenv
# 43) Pull the data file from the S3 bucket using DVC
dvc pull


Misc:
trasnfer learning
appraoch for oneshot learning
https://matthewmcateer.me/blog/using-tpus-in-google-colab/
https://medium.com/@jannik.zuern/using-a-tpu-in-google-colab-54257328d7da

https://colab.research.google.com/drive/1XvhLFD4zECIY6wLQJHDgZORVGGY2GDJa

https://developers.google.com/machine-learning/glossary


https://youtu.be/gkWHGy70jIg
https://youtu.be/gkWHGy70jIg?t=245
https://youtu.be/jm2r5xzYx-A?t=49
https://youtu.be/jm2r5xzYx-A?t=199

Deep learning with Python Book:
Chapter 1: Intro
	Decision Tree -> Random Forest -> Gradient Boosting
	                  (ensembling)

	XGBoosting library for improved gradient decented problems
	https://xgboost.readthedocs.io/en/latest/

	GPU - NVIDIA Titan X approx. $1000 at 2015.
	6.6 TFLOPS of single precision = 6.6 trillion float32 ops/sec

	ImageNet dataset (1.1 million) and 1000 category
	The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), www.image-net.org/challenges/LSVRC

	See “Flexible, High Performance Convolutional Neural Networks for Image Classification,” Proceedings of the
	22nd International Joint Conference on Artificial Intelligence (2011), www.ijcai.org/Proceedings/11/Papers/
	210.pdf.

	See “ImageNet Classification with Deep Convolutional Neural Networks,” Advances in Neural Information Processing
	Systems 25 (2012), http://mng.bz/2286.
	
Chapter 2: Maths behind DL	
	Tensors:
	- scalar or 0D tensor
	  0 axies
		e.g 5
	- vector or 1D tensor
	  1 axies
		e.g [1, 2, 3]
	- matrices or 2D tensor
	  2 axies (rows, columns)
		e.g [[1,2,3],
		     [4,5,6],
				 [7,8,9]]
	- 3D tensor
	  3 axies
		each row contains a matrix
		e.g [m1, m2, m3]
	- Higher dimensional tensors
	  n axies
		each row contains 3D, 4D, 5D, etc.
	- generally we use
	  if data range from 0D to 4D 
	  if it is video processing 5D
	- 3 key attributes
	  - No.of axes (rank)
		  e.g data.ndim
		- shape takes about dimension of tensor
		  e.g data.shape
		- data type float32, float64, uint8, etc


TIMESERIES DATA is a sequence of data based on time
---------------
Example as follows
stock: goog
date: 2020/Oct/25
time interval: 08:00 - 17:00
opening price: 100.21
closing price: 101.19

DATE        TIME         STOCK     PRICE
2020/Oct/25 08:00:00     goog      100.21
2020/Oct/25 08:00:01     goog      100.21
2020/Oct/25 08:00:02     goog      100.21
.                                  
.                                  
.                                  
2020/Oct/25 08:04:21     goog      100.33
.                                  
.                                  
.                                  
2020/Oct/25 08:10:02     goog      100.46
.                                  
.                                  
.                                  
2020/Oct/25 17:00:00     goog      101.19

so end of the day, we have 6.5 * 60 * 60 = 23,400 records

1 day data = 23,400 samples
we are interested in 3 datapoints which is time, stock, price hence it forms 1D tensor = 1 vector of 3D (time, stock, price)

suppose, we want to process a day of data, it must be represented as 2D tensor
[(time, stock, price),
(time, stock, price),
(time, stock, price),
.
.
.
(time, stock, price)]

shape is (23,400, 3)

suppose, we want to process a 250 days data, we have to consider 
including date, hence, our problem expands to processing of 3D tensors
[[day1, <2d tensor data>],
[day2, <2d tensor data>],
[day3, <2d tensor data>],
.
.
.
[day250, <2d tensor data>]]

shape is (250, 23,400, 3)


Image data (4D tensors)
----------
2D representation of an image has width and height
       
       +-------------+
       |             |
       |   |     |   |
       |      |      | height
       |     ___     |
       |             |
       +-------------+
			     width

image attributes
 - width
 - height
 - color channel (greyscale, rgb, hsg)

channel-first = Theano
channel-last  = TensorFlow

Hence, 
TensorFlow library puts color-depth axis at the end (samples, height, width, color_depth).

Theano places the color depth axis right after the batch axis (samples, color_depth, height, width).

Keras supports both formats

Video data (5D tensors)
----------
A video is collection images per second
30fps means 30 frames per second that leads to 30 images needs to be processed if 1 second length of video to be processed by Deep Learning model

image can be stored in 3D tensor (width, height, color_depth)
1sec video can be stored in 4D tensor (frame, width, height, color_depth)
batch of different video is stored in 5D tensor of shape (samples, frame, width, height, color_depth)

For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per
second would have 240 frames. A batch of 4 such video clipse would be stored in a shape of (4, 240, 144, 256, 3)

106,168,320 and if dtype of tensor is float32 (32 bit of memory)
424,673,280 bytes
414,720 kilobytes
405 megabyts

keras.layers.Dense(512, actviation='relu')

for instance, input is 2D tensor, the above function outputs 2D tensor

output = relu(dot(W, input) + b)
W = weight
b = bias

tensor operation
 - Element-wise operation
   - add, sub, mul, div, etc
 - Broadcasting (extending shapes depends on max shape tensor)
 - dot product (leads to scalar value)
 - reshaping
 - transpose
 - geometric interpretation of tensor operation
 - 
 
GEOMETRIC INTERPRETATION OF TENSOR OPERATION 
 (Y)
  ^
  |
  |
 4|
  |          C
 3|         +
  |  A 
 2|   +
  |  /
 1| /     + B
  |/
0 +---------------------------------> (X)
  0  1   2   3   4
	
vector A = [1, 2]
vector B = [2, 1]

A + B = [1, 2] + [2, 1]
      = [3, 3]
			=> C = [3, 3]

In general, elementary geometric operations such as affine transformations, rotations, scaling, and so on can be expressed as tensor operations. For instance, a rotation of a 2D vector by an angle theta can be achieved via a dot product with a 2 × 2 matrix R = [u, v], where u and v are both vectors of the plane: u = [cos(theta), sin(theta)] and v = [-sin(theta), cos(theta)].

A GEOMETRIC INTERPRETATION OF DEEP LEARNING
You just learned that neural networks consist entirely of chains of tensor operations and that all of these tensor operations are just geometric transformations of the input data. It follows that you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps.

Gradient Descent
Stochastic Gradient Descent
 Mini-batch SGD algorithm

optimization address issue of finding global minimum. If on opt, then local minimum will be picked by SGD, our model adjust wrong value to weight

Backpropogation (or reverse-mode differentiation) algorithm
symbolic differentiation such as TensorFlow

Chapter 3: Getting started
Three most common use cases of neural networks: 
 - binary classification
 - multiclass classification
 - scalar regression


https://github.com/plaidml/plaidml
https://vertexai-plaidml.readthedocs-hosted.com/en/latest/usage/install.html
https://mlir.llvm.org


simple dataset: MNIST fashion
complex dataset CIFAR-10

I use PlaidML with Python and Keras on my MacBook Pro (2.2 GHz Intel core i7 processors, 32GB RAM, Radeon Pro 560X GPU), and and get very good results. I find deep learning models run about 20 to 50 times faster than with just the CPU. The only issue I notice is with stability; sometimes PlaidML hangs if I try to use my external monitor while the model is running.


Layers: the building blocks of deep learning
Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers (the Dense class in Keras). Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers such as an LSTM layer. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D).

Models: networks of layers
 Two-branch networks
 Multihead networks
 Inception blocks
The topology of a network defines a hypothesis space.
no rule, by practice, topology decided

Loss functions and optimizers:
Two keys to configuring the learning process
 Loss function (objective function)—The quantity that will be minimized during training. It represents a measure of success for the task at hand.
 Optimizer—Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).
--------------------------+---------------------------------------------
Problem Type                Loss Fn                   
--------------------------+---------------------------------------------
Two-class classification    Binary crossentropy       
Many-class classification   Categorical crossentropy  
Regression                  Mean squared error        
Sequence-learning           Connectionist temporal classification (CTC)
--------------------------+---------------------------------------------

check: From generative adversarial network to a neural Turing machine.
check: NVIDIA CUDA Deep Neural Network library (cuDNN).

http://deeplearning.net/software/theano/
https://www.tensorflow.org
https://github.com/Microsoft/CNTK
http://eigen.tuxfamily.org/index.php?title=Main_Page
http://eigen.tuxfamily.org/dox/GettingStarted.html

All the code examples in this book are available as open source notebooks; you can download them from the book’s website at www.manning.com/books/deep-learning-with-python.

68 page
http://introtodeeplearning.com/
https://github.com/aamini/introtodeeplearning/
https://www.youtube.com/watch?v=njKP3FqW3Sk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=1


The input data is vectors, and the labels are scalars (1s and 0s): this is the easiest setup you’ll ever encounter. A type of network that performs well on such a problem is a simple stack of fully connected (Dense) layers with relu activations: Dense(16, activation='relu').

Visualizing the Loss Landscape of Neural Nets
https://arxiv.org/abs/1712.09913

Gradient descent algorithms:
algorithm TF Implementation
SGD       tf.keras.optimizers.SGD
Adam      tf.keras.optimizers.Adam
Adadelta  tf.keras.optimizers.Adadelta
Adagrad   tf.keras.optimizers.Adagrad
RMSProp   tf.keras.optimizers.RMSProp

papers:
- Kiefer & Wolfowitz, "Stochastic estimation of the maximum of a regression function", 1952
- Kingma et al, "Adam: A method for stochastic Optimazation", 2014
- Zeiler et al, "Adadelta: Adaptive learning rate method" 2012
- Duchi et al, "Adaptive subgradient method for online learing and stochastic optimization", 2011

http://ruder.io/optimizing-gradient-descent
https://arxiv.org/abs/1609.04747

https://deepmind.com
https://openai.com
https://ai.facebook.com
https://ai.google
